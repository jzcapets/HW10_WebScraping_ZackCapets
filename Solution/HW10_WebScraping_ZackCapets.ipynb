{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1><b>Mission to Mars Web Scraping Homework</b></h1>\n",
    "<h3>This notebook uses Selenium/Splinter to visit the NASA Mars Exploration Program Website and scrape the latest news.</h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NASA Article Scraping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import dependencies\n",
    "from bs4 import BeautifulSoup as bs\n",
    "from splinter import Browser\n",
    "import pandas as pd\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Configure Browser\n",
    "executable_path = {'executable_path': 'chromedriver.exe'}\n",
    "browser1 = Browser('chrome', **executable_path, headless=False)\n",
    "url_marsnasa = 'https://mars.nasa.gov/news/?page=0&per_page=40&order=publish_date+desc%2Ccreated_at+desc&search=&category=19%2C165%2C184%2C204&blank_scope=Latest'\n",
    "browser1.visit(url_marsnasa)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#scrape from browser\n",
    "soup = bs(browser1.html,'html.parser')\n",
    "#print(soup.prettify())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Virginia Middle School Student Earns Honor of Naming NASA's Next Mars Rover\""
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#scrape the title of the first headline\n",
    "title_scrape = soup.find_all('div',class_=\"list_text\")\n",
    "topheadline  = title_scrape[0].find_all('div','content_title')[0].text\n",
    "topheadline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'NASA chose a seventh-grader from Virginia as winner of the agency\\'s \"Name the Rover\" essay contest. Alexander Mather\\'s entry for \"Perseverance\" was voted tops among 28,000 entries. '"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#scrape the teaser text for the first article\n",
    "body_scrape = soup.find_all('div',class_=\"list_text\")\n",
    "teaser = body_scrape[0].find_all('div', class_ = 'article_teaser_body')[0].text\n",
    "teaser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "browser1.quit()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# JPL Featured Image Scraping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#set up Browser to go to the JPL site\n",
    "url_marsjplimg = 'https://www.jpl.nasa.gov/spaceimages/?search=&category=Mars'\n",
    "browser2 = Browser('chrome', **executable_path, headless = False)\n",
    "browser2.visit(url_marsjplimg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"background-image: url('/spaceimages/images/wallpaper/PIA19323-1920x1200.jpg');\""
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "soupy = bs(browser2.html, 'html.parser')\n",
    "backgroundimage = soupy.find_all('article')[0]['style']\n",
    "backgroundimage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/spaceimages/images/wallpaper/PIA19323-1920x1200.jpg'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#extract the url from the above result...identify the '/' character as the start of the string, and get rid of the semicolon and endquote\n",
    "imgstring = \"\"\n",
    "nstart = backgroundimage.find('/')\n",
    "nend = len(backgroundimage) - 3\n",
    "\n",
    "\n",
    "\n",
    "for n in range(nstart,nend):\n",
    "    imgstring += backgroundimage[n]\n",
    "\n",
    "imgstring\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'https://www.jpl.nasa.gov/spaceimages/images/wallpaper/PIA19323-1920x1200.jpg'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#concatenate the full image url since the above result is a relative path\n",
    "jpl = 'https://www.jpl.nasa.gov'\n",
    "jplfullurl = jpl + imgstring\n",
    "jplfullurl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "browser2.quit()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Twitter Scraping for Mars Weather Report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#set up Browser for scraping\n",
    "#on this particular twitter page the latest tweet text is stored within the 27th instance of the particular css class\n",
    "url_marsweather = 'https://twitter.com/marswxreport?lang=en'\n",
    "browser3 = Browser('chrome', **executable_path, headless = False)\n",
    "browser3.visit(url_marsweather)\n",
    "\n",
    "#this time delay is necessary to ensure all page resources load before scraping is attempted.\n",
    "    #does splinter have resources that will wait on the specific page resource to load before continuing?\n",
    "#absolutely\n",
    "    #is this effective and will a more elegant solution cost extra and add complication even though this works? \n",
    "#you bet.\n",
    "time.sleep(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#set up a list to hold the weather reports that we will scrape from twitter\n",
    "#set up the keyword that the weather reports start with. If this changes in the future it can be modified here.\n",
    "weatherlist = []\n",
    "keyword = \"InSight\"\n",
    "\n",
    "#this css class includes many items on the page, we want just the weather reports, which all begin with 'InSight'\n",
    "soupier = bs(browser3.html, 'html.parser')\n",
    "weather = soupier.find_all('span', class_ = \"css-901oao css-16my406 r-1qd0xha r-ad9z0x r-bcqeeo r-qvutc0\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#loop through scraped html to find items within this css class matching the keyword, add these to our weather list\n",
    "for n in range(0, len(weather)):\n",
    "    if keyword in weather[n].text:\n",
    "        weatherlist.append(weather[n].text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'InSight sol 457 (2020-03-10) low -95.7ºC (-140.3ºF) high -9.1ºC (15.6ºF) winds from the SSE at 6.5 m/s (14.5 mph) gusting to 21.0 m/s (46.9 mph) pressure at 6.30 hPa'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#just store a reference to the latest one, i.e item 0 in our weather list, strip out the newline characters\n",
    "forecast = weatherlist[0].replace('\\n',' ')\n",
    "forecast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "browser3.quit()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mars Facts Scraping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<table border=\"1\" class=\"dataframe\">\\n  <thead>\\n    <tr style=\"text-align: right;\">\\n      <th>Mars - Earth Comparison</th>\\n      <th>Mars</th>\\n      <th>Earth</th>\\n    </tr>\\n  </thead>\\n  <tbody>\\n    <tr>\\n      <td>Diameter:</td>\\n      <td>6,779 km</td>\\n      <td>12,742 km</td>\\n    </tr>\\n    <tr>\\n      <td>Mass:</td>\\n      <td>6.39 × 10^23 kg</td>\\n      <td>5.97 × 10^24 kg</td>\\n    </tr>\\n    <tr>\\n      <td>Moons:</td>\\n      <td>2</td>\\n      <td>1</td>\\n    </tr>\\n    <tr>\\n      <td>Distance from Sun:</td>\\n      <td>227,943,824 km</td>\\n      <td>149,598,262 km</td>\\n    </tr>\\n    <tr>\\n      <td>Length of Year:</td>\\n      <td>687 Earth days</td>\\n      <td>365.24 days</td>\\n    </tr>\\n    <tr>\\n      <td>Temperature:</td>\\n      <td>-153 to 20 °C</td>\\n      <td>-88 to 58°C</td>\\n    </tr>\\n  </tbody>\\n</table>'"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#the url\n",
    "url_marsfacts = 'https://space-facts.com/mars/'\n",
    "#use pandas to scrape the table data (it's the middle table out of 3)\n",
    "factstable = pd.read_html(url_marsfacts)\n",
    "marsfacts_html = factstable[1].to_html(index = False)\n",
    "marsfacts_html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mars Hemispheres Scraping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#urls we need\n",
    "astrourl = \"https://astrogeology.usgs.gov\"\n",
    "url_hemispherepics = 'https://astrogeology.usgs.gov/search/results?q=hemisphere+enhanced&k1=target&v1=Mars'\n",
    "\n",
    "#configure splinter\n",
    "browser4 = Browser('chrome', **executable_path, headless = False)\n",
    "browser4.visit(url_hemispherepics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#parse html on main mars pics page\n",
    "soupiest = bs(browser4.html, 'html.parser')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#splinter isn't cooperating and won't let me click on links by link text or partial link text\n",
    "#this will scrape a-tags and product-item class which gives us 8 links (including duplicates)\n",
    "linklist1 = []\n",
    "for j in range(0,len(soupiest.find_all('a', class_ = \"product-item\"))):\n",
    "    linklist1.append(soupiest.find_all('a', class_ = \"product-item\")[j]['href'])\n",
    "linklist1\n",
    "\n",
    "#get rid of duplicates -send our list to pandas dataframe, extract uniques, and back to list again in one line! huzzah!\n",
    "links = pd.DataFrame(linklist1, columns = ['link'])['link'].unique().tolist()\n",
    "\n",
    "#set up blank dictionary, lists to hold the scraped titles and urls\n",
    "\n",
    "urllist = []\n",
    "prov1 = []\n",
    "resultdict = {}\n",
    "\n",
    "#loop through the links we scraped for each one, send the browser to that link and get the title and url\n",
    "for k in range(0,len(links)):\n",
    "    #the scraped links above are all relative to the base url for the site\n",
    "    url = astrourl + links[k] \n",
    "    \n",
    "    #go to the url\n",
    "    browser4.visit(url) \n",
    "    \n",
    "    #read the html\n",
    "    soupierest = bs(browser4.html, 'html.parser') \n",
    "    \n",
    "    #get the pic url\n",
    "    url_item = soupierest.find_all('div', class_=\"downloads\")[0].find_all('li')[0].find_all('a')[0]['href'] \n",
    "    urllist.append(url_item)\n",
    "    \n",
    "    #get the title...way easier than getting the pic url\n",
    "    #this finds the word \"Hemisphere\" in the result string and grabs the string including 'Hemisphere'\n",
    "    titletext = soupierest.find_all('title')[0].text \n",
    "    hemispherename = titletext[0:titletext.find(\"Hemisphere\")+10]\n",
    "    prov1.append(hemispherename)\n",
    "    \n",
    "resultdict['title']=prov1\n",
    "resultdict['url']=urllist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "browser4.quit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#test out our scrape dictionary\n",
    "scrapedata = {}\n",
    "scrapedata['headline']=topheadline\n",
    "scrapedata['teaser']=teaser\n",
    "scrapedata['featureimage']=jplfullurl\n",
    "scrapedata['marsfacts']=marsfacts_html\n",
    "scrapedata['pics'] = resultdict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scrapedata['marsfacts']"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
